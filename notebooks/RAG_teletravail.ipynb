{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b55acc-0e5c-4d30-8b60-50070eb0307f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook from https://medium.com/@thakermadhav/build-your-own-rag-with-mistral-7b-and-langchain-97d0c92fa146\n",
    "!pip install -q torch datasets\n",
    "!pip install -q accelerate==0.21.0 \\\n",
    "                peft==0.4.0 \\\n",
    "                bitsandbytes==0.40.2 \\\n",
    "                transformers==4.31.0 \\\n",
    "                trl==0.4.7\n",
    "!pip install -q scipy langchain transformers playwright html2text sentence_transformers faiss-gpu\n",
    "!pip install -q --upgrade git+https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f860479f-f9d0-4162-9701-aeb4da44f3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!playwright install > /dev/null\n",
    "!playwright install-deps > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923862f9-6c52-4509-b85f-b464c7ba0cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import (\n",
    "  AutoTokenizer, \n",
    "  AutoModelForCausalLM, \n",
    "  BitsAndBytesConfig,\n",
    "  pipeline\n",
    ")\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_transformers import Html2TextTransformer\n",
    "from langchain.document_loaders import AsyncChromiumLoader\n",
    "\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "import nest_asyncio\n",
    "#################################################################\n",
    "# Tokenizer\n",
    "#################################################################\n",
    "\n",
    "model_name=\"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "model_name=\"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "#################################################################\n",
    "# bitsandbytes parameters\n",
    "#################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False\n",
    "\n",
    "#################################################################\n",
    "# Set up quantization config\n",
    "#################################################################\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "# Check GPU compatibility with bfloat16\n",
    "if compute_dtype == torch.float16 and use_4bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "#################################################################\n",
    "# Load pre-trained config\n",
    "#################################################################\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    cache_dir=\".\"\n",
    ")\n",
    "#load_in_4bits=True)\n",
    "\n",
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
    "\n",
    "print(print_number_of_trainable_model_parameters(model))\n",
    "\n",
    "text_generation_pipeline = pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    temperature=0.2,\n",
    "    repetition_penalty=1.1,\n",
    "    return_full_text=True,\n",
    "    max_new_tokens=1000,\n",
    ")\n",
    "\n",
    "mistral_llm = HuggingFacePipeline(pipeline=text_generation_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9999bfe4-635e-4500-8d5b-7c150593d40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_a_indexer=\"https://www.droits-salaries.com/349927012-eurogerm/34992701200030-siege/T02121003194-accord-teletravail-2021-teletravail.shtml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8840844-5e85-490b-bdc5-31ba92840b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Articles to index\n",
    "articles = [article_a_indexer]\n",
    "\n",
    "# Scrapes the blogs above\n",
    "loader = AsyncChromiumLoader(articles)\n",
    "docs = loader.load()\n",
    "\n",
    "# Converts HTML to plain text \n",
    "html2text = Html2TextTransformer()\n",
    "docs_transformed = html2text.transform_documents(docs)\n",
    "\n",
    "# Chunk text\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, \n",
    "                                      chunk_overlap=100)\n",
    "chunked_documents = text_splitter.split_documents(docs_transformed)\n",
    "\n",
    "# Load chunked documents into the FAISS index\n",
    "db = FAISS.from_documents(chunked_documents, \n",
    "                          HuggingFaceEmbeddings(model_name='sentence-transformers/all-mpnet-base-v2'))\n",
    "\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23541d08-0c8b-4d2e-b012-0acf63595a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create prompt template\n",
    "prompt_template = \"\"\"\n",
    "### [INST] Instruction: Answer the question based on your french business agreements knowledge. Here is context to help:\n",
    "\n",
    "{context}\n",
    "\n",
    "### QUESTION:\n",
    "{question} [/INST]\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4e7965-223c-49ed-bea1-5c0dd89a6b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create prompt from prompt template \n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=prompt_template,\n",
    ")\n",
    "\n",
    "# Create llm chain \n",
    "llm_chain = LLMChain(llm=mistral_llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ef9454-0ad5-4077-be11-88c104ffb8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = ( \n",
    " {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | llm_chain\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f515ce42-d0fc-4c57-8e4e-54b445f5bbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac995332-94eb-4cec-908a-c6a4f992eeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "reponse= rag_chain.invoke(\"(days_of_remote_work_per_week:float, confidence_score:float) : S'il est fait mention d'une quotité par semaine et sans faire de calcul ni d'extrapolation à partir d'autres quotités, combien de jours maximum un salarié peut-il télétravailler par semaine ? Sinon, renvoyez-moi le tuple (None,0). Le chiffre doit être présent dans l'accord. Je veux exploiter vos réponses, donnez-moi les deux variables dans un tuple sans explication !\")\n",
    "print(reponse[\"text\"])\n",
    "reponse= rag_chain.invoke(\"(days_of_remote_work_per_month:float, confidence_score:float) : S'il est fait mention d'une quotité par mois et sans faire de calcul ni d'extrapolation à partir d'autres quotités, combien de jours maximum un salarié peut-il télétravailler par mois ? Sinon, renvoyez-moi le tuple (None,0). Le chiffre doit être présent dans l'accord. Je veux exploiter vos réponses, donnez-moi les deux variables dans un tuple sans explication !\")\n",
    "print(reponse[\"text\"])\n",
    "reponse= rag_chain.invoke(\"(days_of_remote_work_per_quarter:float, confidence_score:float) : S'il est fait mention d'une quotité par trimestre et sans faire de calcul ni d'extrapolation à partir d'autres quotités, combien de jours maximum un salarié peut-il télétravailler par trimestre ? Sinon, renvoyez-moi le tuple (None,0). Le chiffre doit être présent dans l'accord. Je veux exploiter vos réponses, donnez-moi les deux variables dans un tuple sans explication !\")\n",
    "print(reponse[\"text\"])\n",
    "reponse= rag_chain.invoke(\"(days_of_remote_work_per_year:float, confidence_score:float) : S'il est fait mention d'une quotité par année dans le context et sans faire de calcul ni d'extrapolation à partir d'autres quotités (hebdomadaire, mensuel, trimestriel), combien de jours maximum un salarié peut-il télétravailler par année ? Sinon, renvoyez-moi le tuple (None,0). Le chiffre doit être présent dans l'accord. Je veux exploiter vos réponses, donnez-moi les deux variables dans un tuple sans explication !\")\n",
    "print(reponse[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bb1a2d-46f0-4cd9-81c7-ca1eb4aa7128",
   "metadata": {},
   "outputs": [],
   "source": [
    "reponse= rag_chain.invoke(\"(days_of_remote_work_per_year:float, confidence_score:float) : If the context mentions a quota per year, and without calculating or extrapolating from other quotas (weekly, monthly, quarterly), how many days maximum can an employee telework per year? If not, send me the tuple (None,0). The figure must be present in the agreement. I want to exploit your answers, give me the two variables in a tuple without explanation!\")\n",
    "print(reponse[\"text\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
