{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b55acc-0e5c-4d30-8b60-50070eb0307f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook from https://medium.com/@thakermadhav/build-your-own-rag-with-mistral-7b-and-langchain-97d0c92fa146\n",
    "!pip install -q torch datasets\n",
    "!pip install -q accelerate==0.21.0 \\\n",
    "                peft==0.4.0 \\\n",
    "                bitsandbytes==0.40.2 \\\n",
    "                transformers==4.31.0 \\\n",
    "                trl==0.4.7\n",
    "!pip install -q scipy langchain transformers playwright html2text sentence_transformers faiss-gpu\n",
    "!pip install -q --upgrade git+https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f860479f-f9d0-4162-9701-aeb4da44f3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!playwright install > /dev/null\n",
    "!playwright install-deps > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923862f9-6c52-4509-b85f-b464c7ba0cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import (\n",
    "  AutoTokenizer, \n",
    "  AutoModelForCausalLM, \n",
    "  BitsAndBytesConfig,\n",
    "  pipeline\n",
    ")\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_transformers import Html2TextTransformer\n",
    "from langchain.document_loaders import AsyncChromiumLoader\n",
    "\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "import nest_asyncio\n",
    "#################################################################\n",
    "# Tokenizer\n",
    "#################################################################\n",
    "\n",
    "model_name=\"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "model_name=\"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "#################################################################\n",
    "# bitsandbytes parameters\n",
    "#################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False\n",
    "\n",
    "#################################################################\n",
    "# Set up quantization config\n",
    "#################################################################\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "# Check GPU compatibility with bfloat16\n",
    "if compute_dtype == torch.float16 and use_4bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "#################################################################\n",
    "# Load pre-trained config\n",
    "#################################################################\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    cache_dir=\".\"\n",
    ")\n",
    "#load_in_4bits=True)\n",
    "\n",
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
    "\n",
    "print(print_number_of_trainable_model_parameters(model))\n",
    "\n",
    "text_generation_pipeline = pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    temperature=0.2,\n",
    "    repetition_penalty=1.1,\n",
    "    return_full_text=True,\n",
    "    max_new_tokens=1000,\n",
    ")\n",
    "\n",
    "mistral_llm = HuggingFacePipeline(pipeline=text_generation_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9999bfe4-635e-4500-8d5b-7c150593d40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_a_indexer=\"https://www.droits-salaries.com/420531139-gie-auxia-gestion/42053113900079/T07521028203-accord-relatif-au-teletravail-au-sein-du-gie-auxia-gestion-teletravail.shtml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8840844-5e85-490b-bdc5-31ba92840b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Articles to index\n",
    "articles = [article_a_indexer]\n",
    "\n",
    "# Scrapes the blogs above\n",
    "loader = AsyncChromiumLoader(articles)\n",
    "docs = loader.load()\n",
    "\n",
    "# Converts HTML to plain text \n",
    "html2text = Html2TextTransformer()\n",
    "docs_transformed = html2text.transform_documents(docs)\n",
    "\n",
    "# Chunk text\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, \n",
    "                                      chunk_overlap=100)\n",
    "chunked_documents = text_splitter.split_documents(docs_transformed)\n",
    "\n",
    "# Load chunked documents into the FAISS index\n",
    "db = FAISS.from_documents(chunked_documents, \n",
    "                          HuggingFaceEmbeddings(model_name='sentence-transformers/all-mpnet-base-v2'))\n",
    "\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23541d08-0c8b-4d2e-b012-0acf63595a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "### [INST] Instruction: Answer the question written in French based on your french business agreements knowledge. #MANDATORY : DON'T USE WORDS BUT RETURN ONLY THE ASKED DATA AND DON'T COMPUTE EXTRAPOLATION BETWEEN PERIODS !!!! The answer should only give the number or NA with it confidence score as a tuple (x,y) as it will be used in a datascience project, hence the answer should be directly processable and don't use words nor code snippet. Here is context to help:\n",
    "\n",
    "{context}\n",
    "\n",
    "### QUESTION:\n",
    "{question} [/INST]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065f4c3e-99d2-46ff-b97c-39ba7ffa5433",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "\n",
    "### [ROLE] You are an expert of French business agreements and you are given the task to produce quality data from a business agreement. The answered data should only be a float or NA and nothing more. You are not allowed to use english word, nor code snippet.\n",
    "\n",
    "### [INST] Instruction: Return the asked data from the question. #MANDATORY : DON'T USE WORDS BUT RETURN ONLY THE ASKED DATA AND DON'T COMPUTE EXTRAPOLATION BETWEEN PERIODS !!!! The answer should only give the number or NA  as it will be used in a datascience project, hence the answer should be directly processable and don't use words nor code snippet.\n",
    "\n",
    "Here is context to help:\n",
    "\n",
    "{context}\n",
    "\n",
    "### QUESTION:\n",
    "{question} [/INST]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4e7965-223c-49ed-bea1-5c0dd89a6b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create prompt from prompt template \n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=prompt_template,\n",
    ")\n",
    "\n",
    "# Create llm chain \n",
    "llm_chain = LLMChain(llm=mistral_llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ef9454-0ad5-4077-be11-88c104ffb8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = ( \n",
    " {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | llm_chain\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f515ce42-d0fc-4c57-8e4e-54b445f5bbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738e6397-10db-4fad-9abc-b1780fcbdd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_DUREE=\"S'il est fait mention d'une durée de l'accord, est-ce que l'accord est à durée déterminée ? Oui=1 ou non=0 ou NA?\"\n",
    "Q_REVERS=\"S'il est fait mention d'une clause et d'un article de réversibilité, est-ce que l'accord a une clause ou un article de réversibilité ? Oui=1 ou non=0 ou NA?\"\n",
    "Q_ADAPT=\"S'il est fait mention d'une période d'adaptation, est-ce que l'accord comprend une période d'adaptation ? Oui=1 ou non=0 ou NA?\"\n",
    "Q_TTREG=\"S'il est fait mention d'un télétravail régulier, est-ce que l'accord détaille le télétravail régulier ? Oui=1 ou non=0 ou NA?\"\n",
    "Q_TTOCA=\"S'il est fait mention d'un télétravail occasionnel, est-ce que l'accord mentionne le télétravail occasionnel ? Oui=1 ou non=0 ou NA?\"\n",
    "Q_TTEXC=\"S'il est fait mention d'un télétravail exceptionnel, est-ce que l'accord mentionne le télétravail exceptionnel ? Oui=1 ou non=0 ou NA?\"\n",
    "Q_TTSEM=\"S'il est fait mention d'une quotité par semaine, combien de jours maximum un salarié peut-il télétravailler par semaine ? Attention, il peut être mentionné un nombre de jours de présence hebdomadaire, il faut alors inférer le nombre maximal autorisé de jour de télétravail autorisé sachant qu'il y a 5 jours ouvrés.\"\n",
    "Q_TTMOIS=\"Est-ce que l'accord mentionne d'une quotité de jour de télétravail par mois? Oui=1 ou non=0 ou NA?\"\n",
    "Q_TTTRIM=\"S'il est fait mention d'une quotité par trimestre, combien de jours maximum un salarié peut-il télétravailler par trimestre ?\"\n",
    "Q_TTANNEE=\"S'il est fait mention d'une quotité par an, combien de jours maximum un salarié peut-il télétravailler par an ?\"\n",
    "Q_TTEXCEP=\"S'il est fait mention d'une quotité exceptionelle, combien de jours maximum un salarié peut-il télétravailler par an de manière exceptionelle?\"\n",
    "Q_TTTOTAL=\"Au total, combien de jours maximum un salarié peut-il télétravailler par an?\"\n",
    "Q_EQUIP=\"S'il est fait mention d'un équipement fourni, est-ce que l'accord mentionne un équipement fourni ? Oui=1 ou non=0 ou NA?\"\n",
    "Q_COMPJ=\"S'il est fait mention d'une indemnité journaliere, de combien est l'indemnité forfaitaire par jour de télétravail ?\"\n",
    "Q_COMPM=\"S'il est fait mention d'une indemnité mensuelle, de combien est l'indemnité mensuelle par mois de télétravail ?\"\n",
    "Q_COMPA=\"S'il est fait mention d'une indemnité annuelle, de combien est l'indemnité annuelle par année de télétravail ?\"\n",
    "Q_COMPO=\"S'il est fait mention d'une indemnité autres que journalière, mensuelle ou annuelle, de combien est l'indemnité liée au télétravail ?\"\n",
    "Q_COMPC=\"If an exceptional allowance is mentioned, what is the maximum exceptional allowance for any remote work for covid?\"\n",
    "Q_COMPE=\"S'il est fait mention d'une indemnité d'équipement, de combien est l'indemnité d'équipement maximale liée au télétravail ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56360ff0-f87d-4395-a486-7bf493b84ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "reponse= rag_chain.invoke(Q_DUREE)\n",
    "print(\"Q_DUREE:\",reponse[\"text\"])\n",
    "reponse= rag_chain.invoke(Q_REVERS)\n",
    "print(\"Q_REVERS:\",reponse[\"text\"])\n",
    "reponse= rag_chain.invoke(Q_ADAPT)\n",
    "print(\"Q_ADAPT:\",reponse[\"text\"])\n",
    "reponse= rag_chain.invoke(Q_TTREG)\n",
    "print(\"Q_TTREG:\",reponse[\"text\"])\n",
    "reponse= rag_chain.invoke(Q_TTOCA)\n",
    "print(\"Q_TTOCA:\",reponse[\"text\"])\n",
    "reponse= rag_chain.invoke(Q_TTEXC)\n",
    "print(\"Q_TTEXC:\",reponse[\"text\"])\n",
    "reponse= rag_chain.invoke(Q_TTSEM)\n",
    "print(\"Q_TTSEM:\",reponse[\"text\"])\n",
    "reponse= rag_chain.invoke(Q_TTMOIS)\n",
    "print(\"Q_TTMOIS:\",reponse[\"text\"])\n",
    "reponse= rag_chain.invoke(Q_TTTRIM)\n",
    "print(\"Q_TTTRIM:\",reponse[\"text\"])\n",
    "reponse= rag_chain.invoke(Q_TTANNEE)\n",
    "print(\"Q_TTANNEE:\",reponse[\"text\"])\n",
    "reponse= rag_chain.invoke(Q_TTEXCEP)\n",
    "print(\"Q_TTEXCEP:\",reponse[\"text\"])\n",
    "reponse= rag_chain.invoke(Q_TTTOTAL)\n",
    "print(\"Q_TTTOTAL:\",reponse[\"text\"])\n",
    "reponse= rag_chain.invoke(Q_EQUIP)\n",
    "print(\"Q_EQUIP:\",reponse[\"text\"])\n",
    "reponse= rag_chain.invoke(Q_COMPJ)\n",
    "print(\"Q_COMPJ:\",reponse[\"text\"])\n",
    "reponse= rag_chain.invoke(Q_COMPM)\n",
    "print(\"Q_COMPM:\",reponse[\"text\"])\n",
    "reponse= rag_chain.invoke(Q_COMPA)\n",
    "print(\"Q_COMPA:\",reponse[\"text\"])\n",
    "reponse= rag_chain.invoke(Q_COMPO)\n",
    "print(\"Q_COMPO:\",reponse[\"text\"])\n",
    "reponse= rag_chain.invoke(Q_COMPC)\n",
    "print(\"Q_COMPC:\",reponse[\"text\"])\n",
    "reponse= rag_chain.invoke(Q_COMPE)\n",
    "print(\"Q_COMPE:\",reponse[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16e40a9-9d6a-4aac-a62c-8bb859029e6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a1d2ff-e439-4e1e-a8e6-9357d93b117b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
