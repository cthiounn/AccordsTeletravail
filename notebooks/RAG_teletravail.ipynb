{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b55acc-0e5c-4d30-8b60-50070eb0307f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook from https://medium.com/@thakermadhav/build-your-own-rag-with-mistral-7b-and-langchain-97d0c92fa146\n",
    "!pip install -q torch datasets\n",
    "!pip install -q accelerate==0.21.0 \\\n",
    "                peft==0.4.0 \\\n",
    "                bitsandbytes==0.40.2 \\\n",
    "                transformers==4.31.0 \\\n",
    "                trl==0.4.7\n",
    "!pip install -q scipy langchain transformers playwright html2text sentence_transformers faiss-gpu\n",
    "!pip install -q --upgrade git+https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f860479f-f9d0-4162-9701-aeb4da44f3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!playwright install > /dev/null\n",
    "!playwright install-deps > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923862f9-6c52-4509-b85f-b464c7ba0cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import (\n",
    "  AutoTokenizer, \n",
    "  AutoModelForCausalLM, \n",
    "  BitsAndBytesConfig,\n",
    "  pipeline\n",
    ")\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_transformers import Html2TextTransformer\n",
    "from langchain.document_loaders import AsyncChromiumLoader\n",
    "\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "import nest_asyncio\n",
    "#################################################################\n",
    "# Tokenizer\n",
    "#################################################################\n",
    "\n",
    "model_name=\"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "model_name=\"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "#################################################################\n",
    "# bitsandbytes parameters\n",
    "#################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False\n",
    "\n",
    "#################################################################\n",
    "# Set up quantization config\n",
    "#################################################################\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "# Check GPU compatibility with bfloat16\n",
    "if compute_dtype == torch.float16 and use_4bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "#################################################################\n",
    "# Load pre-trained config\n",
    "#################################################################\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    cache_dir=\".\"\n",
    ")\n",
    "#load_in_4bits=True)\n",
    "\n",
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
    "\n",
    "print(print_number_of_trainable_model_parameters(model))\n",
    "\n",
    "text_generation_pipeline = pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    temperature=0.2,\n",
    "    repetition_penalty=1.1,\n",
    "    return_full_text=True,\n",
    "    max_new_tokens=1000,\n",
    ")\n",
    "\n",
    "mistral_llm = HuggingFacePipeline(pipeline=text_generation_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9999bfe4-635e-4500-8d5b-7c150593d40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_a_indexer=\"https://www.droits-salaries.com/420531139-gie-auxia-gestion/42053113900079/T07521028203-accord-relatif-au-teletravail-au-sein-du-gie-auxia-gestion-teletravail.shtml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8840844-5e85-490b-bdc5-31ba92840b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Articles to index\n",
    "articles = [article_a_indexer]\n",
    "\n",
    "# Scrapes the blogs above\n",
    "loader = AsyncChromiumLoader(articles)\n",
    "docs = loader.load()\n",
    "\n",
    "# Converts HTML to plain text \n",
    "html2text = Html2TextTransformer()\n",
    "docs_transformed = html2text.transform_documents(docs)\n",
    "\n",
    "# Chunk text\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, \n",
    "                                      chunk_overlap=100)\n",
    "chunked_documents = text_splitter.split_documents(docs_transformed)\n",
    "\n",
    "# Load chunked documents into the FAISS index\n",
    "db = FAISS.from_documents(chunked_documents, \n",
    "                          HuggingFaceEmbeddings(model_name='sentence-transformers/all-mpnet-base-v2'))\n",
    "\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23541d08-0c8b-4d2e-b012-0acf63595a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "### [INST] Instruction: Answer the question written in French based on your french business agreements knowledge. #MANDATORY : DON'T USE WORDS BUT RETURN ONLY THE ASKED DATA AND DON'T COMPUTE EXTRAPOLATION BETWEEN PERIODS !!!! The answer should only give the number or NA with it confidence score as a tuple (x,y) as it will be used in a datascience project, hence the answer should be directly processable and don't use words nor code snippet. Here is context to help:\n",
    "\n",
    "{context}\n",
    "\n",
    "### QUESTION:\n",
    "{question} [/INST]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065f4c3e-99d2-46ff-b97c-39ba7ffa5433",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "\n",
    "### [ROLE] You are an expert of French business agreements and you are given the task to produce quality data from a business agreement without given explanation since you are an expert. You can't use division or multiplication operation in your logic and data should be in the text or infered. The answer is limited at 10 characters. The answered data should only be a float or NA and nothing more. You are not allowed to use english word, nor code snippet.\n",
    "\n",
    "### [INST] Instruction: Return the asked data from the question. #MANDATORY : DON'T USE WORDS BUT RETURN ONLY THE ASKED DATA AND DON'T COMPUTE EXTRAPOLATION BETWEEN PERIODS !!!! The answer should only give the number or NA  as it will be used in a datascience project, hence the answer should be directly processable and don't use words nor code snippet.\n",
    "\n",
    "Here is context to help:\n",
    "\n",
    "{context}\n",
    "\n",
    "### QUESTION:\n",
    "{question} [/INST]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4e7965-223c-49ed-bea1-5c0dd89a6b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create prompt from prompt template \n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=prompt_template,\n",
    ")\n",
    "\n",
    "# Create llm chain \n",
    "llm_chain = LLMChain(llm=mistral_llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ef9454-0ad5-4077-be11-88c104ffb8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = ( \n",
    " {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | llm_chain\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f515ce42-d0fc-4c57-8e4e-54b445f5bbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738e6397-10db-4fad-9abc-b1780fcbdd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_DUREE=\"If a duration is mentioned, is the agreement for a fixed term? Yes=1 or no=0 or NA?\"\n",
    "Q_REVERS=\"If a reversibility clause and article are mentioned, does the agreement have a reversibility clause or article? Yes=1 or no=0 or NA?\"\n",
    "Q_ADAPT=\"If an adaptation period is mentioned, does the agreement include an adaptation period? Yes=1 or no=0 or NA?\"\n",
    "Q_TTREG=\"If regular telework (per week) is mentioned, does the agreement detail regular telework? Yes=1 or no=0 or NA?\"\n",
    "Q_TTOCA=\"If occasional telework (per month or annual) is mentioned, does the agreement mention occasional telework? Yes=1 or no=0 or NA?\"\n",
    "Q_TTEXC=\"If exceptional telework (exceptional circumstances) is mentioned, does the agreement mention exceptional telework? Yes=1 or no=0 or NA?\"\n",
    "Q_TTSEM=\"employee=any employee full-time or part-time combined; If the agreement mentions a number of days per week, how many days maximum can an employee telework per week on a regular basis WITHOUT using any exceptional days of telework? Note that a number of on-site days per week may be mentioned, in which case the maximum number of teleworking days allowed must be inferred, given that there are 5 working days per week.\"\n",
    "Q_TTMOIS=\"employee=any employee full-time or part-time combined; If it mentions a quotient per month, how many days maximum can an employee telework per month WITHOUT using any exceptional days of telework?\"\n",
    "Q_TTTRIM=\"employee=any employee full-time or part-time combined; If it mentions a quotient per quarter, how many days maximum can an employee telework per quarter WITHOUT using any exceptional days of telework?\"\n",
    "Q_TTANNEE=\"employee=any employee full-time or part-time combined; If a quota per year is mentioned, how many days maximum can an employee telework per year WITHOUT using any exceptional days of telework?\"\n",
    "Q_TTEXCEP=\"employee=any employee full-time or part-time combined; If an exceptional quota (modulable days for any employee full-time or part-time combined) is mentioned, how many days maximum can an employee telework per year on an exceptional basis?\"\n",
    "Q_TTTOTAL=\"For this question, you can multiply and infer logic; In total, how many days maximum can an employee telework per year?\"\n",
    "Q_EQUIP=\"If there is mention of equipment provided, does the agreement mention equipment provided? Yes=1 or no=0 or NA?\"\n",
    "Q_COMPJ=\"If a daily allowance (french key words = ['indemnité','remboursement de frais','subvention', 'participation aux frais']) is mentioned, how much is the fixed allowance per day of telework?\"\n",
    "Q_COMPM=\"If a monthly allowance (french key words = ['indemnité','remboursement de frais','subvention', 'participation aux frais']) is mentioned, how much is the monthly allowance per month of telework?\"\n",
    "Q_COMPA=\"If an annual allowance (french key words = ['indemnité','remboursement de frais','subvention', 'participation aux frais']) is mentioned, how much is the annual allowance per year of teleworking?\"\n",
    "Q_COMPO=\"If an allowance (french key words = ['indemnité','remboursement de frais','subvention', 'participation aux frais']) other than daily, monthly or annual is mentioned, how much is the telework-related allowance?\"\n",
    "Q_COMPC=\"If an exceptional allowance (french key words = ['indemnité','remboursement de frais','subvention', 'participation aux frais']) is mentioned, what is the maximum exceptional allowance for any remote work for covid?\"\n",
    "Q_COMPE=\"If an equipment allowance (french key words = ['indemnité','remboursement de frais','subvention', 'participation aux frais']) is mentioned, what is the maximum equipment allowance for any remote work for covid?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56360ff0-f87d-4395-a486-7bf493b84ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "reponse= rag_chain.invoke(Q_DUREE)\n",
    "print(\"Q_DUREE:\",reponse[\"text\"])\n",
    "reponse= rag_chain.invoke(Q_REVERS)\n",
    "print(\"Q_REVERS:\",reponse[\"text\"])\n",
    "reponse= rag_chain.invoke(Q_ADAPT)\n",
    "print(\"Q_ADAPT:\",reponse[\"text\"])\n",
    "reponse= rag_chain.invoke(Q_TTREG)\n",
    "print(\"Q_TTREG:\",reponse[\"text\"])\n",
    "reponse= rag_chain.invoke(Q_TTOCA)\n",
    "print(\"Q_TTOCA:\",reponse[\"text\"])\n",
    "reponse= rag_chain.invoke(Q_TTEXC)\n",
    "print(\"Q_TTEXC:\",reponse[\"text\"])\n",
    "reponse= rag_chain.invoke(Q_TTSEM)\n",
    "print(\"Q_TTSEM:\",reponse[\"text\"])\n",
    "reponse= rag_chain.invoke(Q_TTMOIS)\n",
    "print(\"Q_TTMOIS:\",reponse[\"text\"])\n",
    "reponse= rag_chain.invoke(Q_TTTRIM)\n",
    "print(\"Q_TTTRIM:\",reponse[\"text\"])\n",
    "reponse= rag_chain.invoke(Q_TTANNEE)\n",
    "print(\"Q_TTANNEE:\",reponse[\"text\"])\n",
    "reponse= rag_chain.invoke(Q_TTEXCEP)\n",
    "print(\"Q_TTEXCEP:\",reponse[\"text\"])\n",
    "reponse= rag_chain.invoke(Q_TTTOTAL)\n",
    "print(\"Q_TTTOTAL:\",reponse[\"text\"])\n",
    "reponse= rag_chain.invoke(Q_EQUIP)\n",
    "print(\"Q_EQUIP:\",reponse[\"text\"])\n",
    "reponse= rag_chain.invoke(Q_COMPJ)\n",
    "print(\"Q_COMPJ:\",reponse[\"text\"])\n",
    "reponse= rag_chain.invoke(Q_COMPM)\n",
    "print(\"Q_COMPM:\",reponse[\"text\"])\n",
    "reponse= rag_chain.invoke(Q_COMPA)\n",
    "print(\"Q_COMPA:\",reponse[\"text\"])\n",
    "reponse= rag_chain.invoke(Q_COMPO)\n",
    "print(\"Q_COMPO:\",reponse[\"text\"])\n",
    "reponse= rag_chain.invoke(Q_COMPC)\n",
    "print(\"Q_COMPC:\",reponse[\"text\"])\n",
    "reponse= rag_chain.invoke(Q_COMPE)\n",
    "print(\"Q_COMPE:\",reponse[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16e40a9-9d6a-4aac-a62c-8bb859029e6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a1d2ff-e439-4e1e-a8e6-9357d93b117b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
